{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "def make_dir(path):\n",
    "    if os.path.exists(path) is False:\n",
    "        os.makedirs(path)\n",
    "\n",
    "def evaluate_prediction(predictions, actual, model_name):\n",
    "    errors = predictions - actual\n",
    "    mse = np.square(errors).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.abs(errors).mean()\n",
    "    print('MAE: {:.2f}'.format(mae))\n",
    "    print('RMSE: {:.2f}'.format(rmse))\n",
    "    print('')\n",
    "    print('')\n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = pd.read_csv('dataset/SCED (5m).csv')\n",
    "sl['sced_time_stamp_local'] = pd.to_datetime(sl['sced_time_stamp_local'])\n",
    "sl.set_index('sced_time_stamp_local', inplace=True)\n",
    "sl = sl.resample('h').mean()\n",
    "date_range = pd.date_range(start=sl.index.min(), end=sl.index.max(), freq='h')\n",
    "sl = sl[~sl.index.duplicated(keep='first')]\n",
    "sl = sl.reindex(date_range, fill_value=np.nan)\n",
    "sl.interpolate(method='time', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kv/ldp2ch2s1md4v31v75w2dt2c0000gn/T/ipykernel_4280/1171035437.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dap['timestamp'] = pd.to_datetime(dap['timestamp'])\n"
     ]
    }
   ],
   "source": [
    "dap = pd.read_csv('dataset/DAP (1hr).csv')\n",
    "dap['timestamp'] = pd.to_datetime(dap['timestamp'])\n",
    "dap.set_index('timestamp', inplace=True)\n",
    "date_range = pd.date_range(start=dap.index.min(), end='2024-01-01 23:55:00', freq='h')\n",
    "dap = dap[~dap.index.duplicated(keep='first')]\n",
    "dap = dap.reindex(date_range, fill_value=np.nan)\n",
    "dap.interpolate(method='time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted_tlf = pd.read_csv('dataset/FORECASTED_TLF (15m).csv')\n",
    "forecasted_tlf['datetime'] = pd.to_datetime(forecasted_tlf['datetime'])\n",
    "forecasted_tlf.set_index('datetime', inplace=True)\n",
    "date_range = pd.date_range(start=forecasted_tlf.index.min(), end='2024-01-01 23:55:00', freq='h')\n",
    "forecasted_tlf = forecasted_tlf[~forecasted_tlf.index.duplicated(keep='first')]\n",
    "forecasted_tlf = forecasted_tlf.reindex(date_range, fill_value=np.nan)\n",
    "forecasted_tlf.interpolate(method='time', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAP</th>\n",
       "      <th>SCED</th>\n",
       "      <th>F_TLF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02 00:00:00</th>\n",
       "      <td>23.9250</td>\n",
       "      <td>26.216109</td>\n",
       "      <td>0.016499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02 01:00:00</th>\n",
       "      <td>23.3140</td>\n",
       "      <td>25.671136</td>\n",
       "      <td>0.016441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02 02:00:00</th>\n",
       "      <td>23.3475</td>\n",
       "      <td>24.970952</td>\n",
       "      <td>0.016411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02 03:00:00</th>\n",
       "      <td>23.0595</td>\n",
       "      <td>24.787712</td>\n",
       "      <td>0.016441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02 04:00:00</th>\n",
       "      <td>25.2672</td>\n",
       "      <td>24.887786</td>\n",
       "      <td>0.016534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 19:00:00</th>\n",
       "      <td>19.3715</td>\n",
       "      <td>14.422107</td>\n",
       "      <td>0.021356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 20:00:00</th>\n",
       "      <td>16.7739</td>\n",
       "      <td>12.597410</td>\n",
       "      <td>0.021367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 21:00:00</th>\n",
       "      <td>15.0035</td>\n",
       "      <td>10.170172</td>\n",
       "      <td>0.021383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 22:00:00</th>\n",
       "      <td>15.7699</td>\n",
       "      <td>9.872549</td>\n",
       "      <td>0.021408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 23:00:00</th>\n",
       "      <td>16.2404</td>\n",
       "      <td>12.594126</td>\n",
       "      <td>0.021436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         DAP       SCED     F_TLF\n",
       "2019-01-02 00:00:00  23.9250  26.216109  0.016499\n",
       "2019-01-02 01:00:00  23.3140  25.671136  0.016441\n",
       "2019-01-02 02:00:00  23.3475  24.970952  0.016411\n",
       "2019-01-02 03:00:00  23.0595  24.787712  0.016441\n",
       "2019-01-02 04:00:00  25.2672  24.887786  0.016534\n",
       "...                      ...        ...       ...\n",
       "2023-12-31 19:00:00  19.3715  14.422107  0.021356\n",
       "2023-12-31 20:00:00  16.7739  12.597410  0.021367\n",
       "2023-12-31 21:00:00  15.0035  10.170172  0.021383\n",
       "2023-12-31 22:00:00  15.7699   9.872549  0.021408\n",
       "2023-12-31 23:00:00  16.2404  12.594126  0.021436\n",
       "\n",
       "[43800 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare the data for training and testing\n",
    "df = pd.concat([dap, sl, forecasted_tlf], axis=1)\n",
    "df.columns = ['DAP', 'SCED', 'F_TLF']\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26233, 24, 3) (26233, 24, 1) (8713, 24, 3) (8713, 24, 1) (8713, 24, 3) (8713, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## log transform\n",
    "log_data = df.copy(deep=True)\n",
    "log_data.loc[:,\"SCED\"] = np.log(df.loc[:,\"SCED\"] + 1 - min(df.loc[:,\"SCED\"]))\n",
    "log_data.loc[:,\"DAP\"] = np.log(df.loc[:,\"DAP\"] + 1 - min(df.loc[:,\"DAP\"]))\n",
    "log_data.loc[:,\"F_TLF\"] = np.log(df.loc[:,\"F_TLF\"] + 1 - min(df.loc[:,\"F_TLF\"]))\n",
    "\n",
    "#Shift for prediction\n",
    "df['DAP'] = df['DAP'].shift(-24)\n",
    "\n",
    "# 3 years training, 1 year validation, 1 year testing\n",
    "x_train_df_reg = log_data.loc[:'2021-12-31 23:55:00'].iloc[:,:]\n",
    "x_val_df_reg = log_data.loc['2022-01-01 00:00:00':'2022-12-31 23:55:00'].iloc[:,:]\n",
    "x_test_df_reg = log_data.loc['2023-01-01 00:00:00':].iloc[:,:]\n",
    "\n",
    "# Shift back for target variable\n",
    "df['DAP'] = df['DAP'].shift(24)\n",
    "\n",
    "#Change target variable to DAP\n",
    "y_train_df_reg = log_data.loc[:'2021-12-31 23:55:00'].iloc[:, :1]\n",
    "y_val_df_reg = log_data.loc['2022-01-01 00:00:00':'2022-12-31 23:55:00'].iloc[:, :1]\n",
    "y_test_df_reg = log_data.loc['2023-01-01 00:00:00':].iloc[:, :1]\n",
    "\n",
    "x_train_df_reg.reset_index(drop=True, inplace=True)\n",
    "x_val_df_reg.reset_index(drop=True, inplace=True)\n",
    "x_test_df_reg.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Standardization\n",
    "x_mean_reg, x_std_reg = x_train_df_reg.mean(), x_train_df_reg.std()\n",
    "y_mean_reg, y_std_reg = y_train_df_reg.mean(), y_train_df_reg.std()\n",
    "\n",
    "x_std_reg = x_std_reg +0.00001\n",
    "\n",
    "x_train_reg = (x_train_df_reg - x_mean_reg)/x_std_reg\n",
    "x_val_reg = (x_val_df_reg - x_mean_reg)/x_std_reg\n",
    "x_test_reg = (x_test_df_reg - x_mean_reg)/x_std_reg\n",
    "\n",
    "y_train_reg = (y_train_df_reg - y_mean_reg)/y_std_reg\n",
    "y_val_reg = (y_val_df_reg - y_mean_reg)/y_std_reg\n",
    "y_test_reg = (y_test_df_reg - y_mean_reg)/y_std_reg\n",
    "\n",
    "# Shift the data for the lags\n",
    "n_steps_in = 24\n",
    "n_steps_out = 24\n",
    "\n",
    "x_train_lstm = np.array([x_train_reg[i:i+n_steps_in] for i in range(0, x_train_reg.shape[0]-n_steps_in-n_steps_out+1)])\n",
    "y_train_lstm = np.array([y_train_reg[i+n_steps_in:i+n_steps_in+n_steps_out] for i in range(0, y_train_reg.shape[0]-n_steps_in-n_steps_out+1)])\n",
    "\n",
    "x_val_lstm = np.array([x_val_reg[i:i+n_steps_in] for i in range(0, x_val_reg.shape[0]-n_steps_in-n_steps_out+1)])\n",
    "y_val_lstm = np.array([y_val_reg[i+n_steps_in:i+n_steps_in+n_steps_out] for i in range(0, y_val_reg.shape[0]-n_steps_in-n_steps_out+1)])\n",
    "\n",
    "x_test_lstm = np.array([x_test_reg[i:i+n_steps_in] for i in range(0, x_test_reg.shape[0]-n_steps_in-n_steps_out+1)])\n",
    "y_test_lstm = np.array([y_test_reg[i+n_steps_in:i+n_steps_in+n_steps_out] for i in range(0, y_test_reg.shape[0]-n_steps_in-n_steps_out+1)])\n",
    "\n",
    "print(x_train_lstm.shape,y_train_lstm.shape,x_val_lstm.shape,y_val_lstm.shape, x_test_lstm.shape,y_test_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The layer sequential has never been called and thus has no defined output.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1) Suppose the final shape is (None, 24). We pick horizon 0:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m sub_output = tf.keras.layers.Lambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[:, \u001b[32m0\u001b[39m])(\u001b[43mlstm_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 2) Build the sub-model\u001b[39;00m\n\u001b[32m      9\u001b[39m sub_model = tf.keras.Model(inputs=lstm_model.input, outputs=sub_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConcordNewEnergy/venv/lib/python3.12/site-packages/keras/src/ops/operation.py:280\u001b[39m, in \u001b[36mOperation.output\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moutput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    272\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Retrieves the output tensor(s) of a layer.\u001b[39;00m\n\u001b[32m    273\u001b[39m \n\u001b[32m    274\u001b[39m \u001b[33;03m    Only returns the tensor(s) corresponding to the *first time*\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m \u001b[33;03m        Output tensor or list of output tensors.\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_node_attribute_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_tensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConcordNewEnergy/venv/lib/python3.12/site-packages/keras/src/ops/operation.py:299\u001b[39m, in \u001b[36mOperation._get_node_attribute_at_index\u001b[39m\u001b[34m(self, node_index, attr, attr_name)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001b[39;00m\n\u001b[32m    284\u001b[39m \n\u001b[32m    285\u001b[39m \u001b[33;03mThis is used to implement the properties:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    296\u001b[39m \u001b[33;03m    The operation's attribute `attr` at the node of index `node_index`.\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inbound_nodes:\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    300\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has never been called \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    301\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mand thus has no defined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    302\u001b[39m     )\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._inbound_nodes) > node_index:\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    305\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAsked to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at node \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    306\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but the operation has only \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._inbound_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m inbound nodes.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: The layer sequential has never been called and thus has no defined output."
     ]
    }
   ],
   "source": [
    "# set hyperparameters\n",
    "n_neurons  = 64  # number of neurons in the Dense layer\n",
    "activation     = 'relu' # activation function\n",
    "learning_rate  = 0.0005\n",
    "minibatch_size = 64\n",
    "num_epochs     = 50\n",
    "\n",
    "# MLP model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(n_neurons,input_shape=(x_train_lstm.shape[1],x_train_lstm.shape[2]),\n",
    "               return_sequences=True,activation=activation))\n",
    "lstm_model.add(LSTM(n_neurons,return_sequences=False,\n",
    "               activation=activation))\n",
    "lstm_model.add(Dense(n_neurons,activation=activation))\n",
    "lstm_model.add(Dense(y_train_lstm.shape[-2],activation='linear')) \n",
    "\n",
    "lstm_model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "\n",
    "lstm_model.summary()\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = lstm_model.fit(x_train_lstm, y_train_lstm, \n",
    "                        validation_data = (x_val_lstm, y_val_lstm), \n",
    "                        batch_size = minibatch_size,\n",
    "                        epochs = num_epochs,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stop],\n",
    "                        shuffle=False)\n",
    "\n",
    "# saving trained model\n",
    "model_path = os.path.join(cwd,'saved_model')\n",
    "make_dir(model_path)\n",
    "lstm_model.save(os.path.join(model_path,'DAP_Model_TLF.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "MAE: 0.54\n",
      "RMSE: 1.01\n",
      "\n",
      "\n",
      "MAE: 46.69\n",
      "RMSE: 203.58\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(46.68796075606509), np.float64(203.5837166941207))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the saved model\n",
    "model_path = os.path.join(cwd,'saved_model')\n",
    "lstm_model = load_model(\n",
    "    os.path.join(model_path, 'DAP_Model_TLF.h5'),\n",
    "    custom_objects={'mse': MeanSquaredError()}\n",
    ")\n",
    "\n",
    "# Make prediciton\n",
    "y_test_pred = lstm_model.predict(x_test_lstm)\n",
    "\n",
    "# Evaluate Prediction\n",
    "evaluate_prediction(y_test_pred , y_test_lstm[:,:,0], 'lstm')\n",
    "\n",
    "# Rescale to get values before normalization\n",
    "y_test_pred_rescale = y_test_pred*y_std_reg.values + y_mean_reg.values\n",
    "y_test_lstm_rescale = y_test_lstm*y_std_reg.values + y_mean_reg.values\n",
    "\n",
    "# inverse log to get prices in the actual scale\n",
    "y_test_pred_invlog = np.exp(y_test_pred_rescale) -1 + min(df.loc[:,\"SCED\"])\n",
    "y_test_lstm_invlog = np.exp(y_test_lstm_rescale) -1 + min(df.loc[:,\"SCED\"])\n",
    "\n",
    "# revaluation after the rescaling\n",
    "evaluate_prediction(y_test_pred_invlog , y_test_lstm_invlog[:,:,0], 'lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM model input shape: (None, 24, 3)\n",
      "LSTM model output shape: (None, 24)\n",
      "Built output shape: (None, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_85\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_85\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">17,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,560</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m3\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m17,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │         \u001b[38;5;34m1,560\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda_14 (\u001b[38;5;33mLambda\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m)                 │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,152</span> (219.34 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m56,152\u001b[0m (219.34 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,152</span> (219.34 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m56,152\u001b[0m (219.34 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import shap\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# Load the model (ensure cwd and x_train_lstm are defined)\n",
    "model_path = os.path.join(cwd, 'saved_model', 'DAP_Model_TLF.h5')\n",
    "lstm_model = load_model(model_path, custom_objects={'mse': MeanSquaredError()})\n",
    "\n",
    "print(\"LSTM model input shape:\", lstm_model.input_shape)\n",
    "print(\"LSTM model output shape:\", lstm_model.output_shape)\n",
    "# Expected: input shape -> (None, 24, num_features)\n",
    "#           output shape -> (None, 24)\n",
    "\n",
    "# Build the model if necessary\n",
    "lstm_model.build(input_shape=(None, x_train_lstm.shape[1], x_train_lstm.shape[2]))\n",
    "print(\"Built output shape:\", lstm_model.output_shape)\n",
    "\n",
    "# Create a sub-model that extracts the first forecast horizon (index 0)\n",
    "class SliceFirst(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return inputs[:, 0]\n",
    "\n",
    "sub_output = tf.keras.layers.Lambda(lambda x: x[:, 0])(lstm_model.outputs[0])\n",
    "sub_model = tf.keras.Model(inputs=lstm_model.inputs, outputs=sub_output)\n",
    "\n",
    "sub_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Sub-model prediction: [-0.05941072]\n"
     ]
    }
   ],
   "source": [
    "# Test the sub-model on a sample from x_train_lstm\n",
    "sample_input = x_train_lstm[:1]  # one sample\n",
    "print(\"Sub-model prediction:\", sub_model.predict(sample_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "StagingError",
     "evalue": "in user code:\n\n    File \"/Users/natowei/ConcordNewEnergy/venv/lib/python3.12/site-packages/shap/explainers/_deep/deep_tf.py\", line 265, in grad_graph  *\n        x_grad = tape.gradient(out, shap_rAnD)\n\n    LookupError: gradient registry has no entry for: shap_TensorListStack\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStagingError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m num_test_samples = \u001b[32m10\u001b[39m\n\u001b[32m     14\u001b[39m test_samples = x_train_lstm[:num_test_samples]\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m shap_values = \u001b[43mexplainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Note: shap_values is a list. For your sub-model that outputs a single value per sample,\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# shap_values[0] will have shape (num_test_samples, 24, num_features)\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 4. Visualize the SHAP values using a summary plot\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# This plot will show the contribution of each feature across all timesteps for each sample.\u001b[39;00m\n\u001b[32m     22\u001b[39m shap.summary_plot(shap_values[\u001b[32m0\u001b[39m], test_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConcordNewEnergy/venv/lib/python3.12/site-packages/shap/explainers/_deep/__init__.py:164\u001b[39m, in \u001b[36mDeepExplainer.shap_values\u001b[39m\u001b[34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs=\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order=\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m, check_additivity=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    121\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m    123\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexplainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConcordNewEnergy/venv/lib/python3.12/site-packages/shap/explainers/_deep/deep_tf.py:332\u001b[39m, in \u001b[36mTFDeep.shap_values\u001b[39m\u001b[34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# run attribution computation graph\u001b[39;00m\n\u001b[32m    331\u001b[39m feature_ind = model_output_ranks[j, i]\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m sample_phis = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mphi_symbolic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_ind\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConcordNewEnergy/venv/lib/python3.12/site-packages/shap/explainers/_deep/deep_tf.py:389\u001b[39m, in \u001b[36mTFDeep.run\u001b[39m\u001b[34m(self, out, model_inputs, X)\u001b[39m\n\u001b[32m    385\u001b[39m         tf_execute.record_gradient = tf_backprop.record_gradient\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m final_out\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_with_overridden_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43manon\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConcordNewEnergy/venv/lib/python3.12/site-packages/shap/explainers/_deep/deep_tf.py:421\u001b[39m, in \u001b[36mTFDeep.execute_with_overridden_gradients\u001b[39m\u001b[34m(self, f)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;66;03m# define the computation graph for the attribution values using a custom gradient-like computation\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     out = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;66;03m# reinstate the backpropagatable check\u001b[39;00m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tf_gradients_impl, \u001b[33m\"\u001b[39m\u001b[33m_IsBackpropagatable\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConcordNewEnergy/venv/lib/python3.12/site-packages/shap/explainers/_deep/deep_tf.py:381\u001b[39m, in \u001b[36mTFDeep.run.<locals>.anon\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    379\u001b[39m     v = tf.constant(data, dtype=\u001b[38;5;28mself\u001b[39m.model_inputs[i].dtype)\n\u001b[32m    380\u001b[39m     inputs.append(v)\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m final_out = \u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    383\u001b[39m     tf_execute.record_gradient = tf_backprop._record_gradient\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConcordNewEnergy/venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConcordNewEnergy/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[39m, in \u001b[36mpy_func_from_autograph.<locals>.autograph_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m     51\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mag_error_metadata\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.ag_error_metadata.to_exception(e)\n\u001b[32m     53\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mStagingError\u001b[39m: in user code:\n\n    File \"/Users/natowei/ConcordNewEnergy/venv/lib/python3.12/site-packages/shap/explainers/_deep/deep_tf.py\", line 265, in grad_graph  *\n        x_grad = tape.gradient(out, shap_rAnD)\n\n    LookupError: gradient registry has no entry for: shap_TensorListStack\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "# 1. Select a background dataset (e.g., 100 random samples from x_train_lstm)\n",
    "num_background = 100\n",
    "background_idx = np.random.choice(x_train_lstm.shape[0], num_background, replace=False)\n",
    "background = x_train_lstm[background_idx]\n",
    "\n",
    "# 2. Create a DeepExplainer using the sub-model and the background dataset\n",
    "explainer = shap.DeepExplainer(sub_model, background)\n",
    "\n",
    "# 3. Compute SHAP values for a set of test samples (e.g., first 10 samples)\n",
    "num_test_samples = 10\n",
    "test_samples = x_train_lstm[:num_test_samples]\n",
    "shap_values = explainer.shap_values(test_samples)\n",
    "\n",
    "# Note: shap_values is a list. For your sub-model that outputs a single value per sample,\n",
    "# shap_values[0] will have shape (num_test_samples, 24, num_features)\n",
    "\n",
    "# 4. Visualize the SHAP values using a summary plot\n",
    "# This plot will show the contribution of each feature across all timesteps for each sample.\n",
    "shap.summary_plot(shap_values[0], test_samples)\n",
    "\n",
    "# Optional: If you wish to aggregate feature importance over the 24 timesteps,\n",
    "# you could compute the mean absolute SHAP value for each feature across timesteps.\n",
    "mean_abs_shap = np.mean(np.abs(shap_values[0]), axis=1)  # shape: (num_test_samples, num_features)\n",
    "import pandas as pd\n",
    "feature_names = [f'Feature {i}' for i in range(x_train_lstm.shape[2])]  # update with your actual feature names if available\n",
    "\n",
    "# Create a DataFrame for one sample (or average across samples) and plot\n",
    "df_shap = pd.DataFrame(mean_abs_shap, columns=feature_names)\n",
    "print(df_shap)\n",
    "df_shap.mean(axis=0).plot(kind='bar', title='Mean Absolute SHAP Value per Feature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
