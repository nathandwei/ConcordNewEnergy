{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "def make_dir(path):\n",
    "    if os.path.exists(path) is False:\n",
    "        os.makedirs(path)\n",
    "\n",
    "def evaluate_prediction(predictions, actual, model_name):\n",
    "    errors = predictions - actual\n",
    "    mse = np.square(errors).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.abs(errors).mean()\n",
    "    print('MAE: {:.2f}'.format(mae))\n",
    "    print('RMSE: {:.2f}'.format(rmse))\n",
    "    print('')\n",
    "    print('')\n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = pd.read_csv('dataset/SCED (5m).csv')\n",
    "sl['sced_time_stamp_local'] = pd.to_datetime(sl['sced_time_stamp_local'])\n",
    "sl.set_index('sced_time_stamp_local', inplace=True)\n",
    "sl = sl.resample('h').mean()\n",
    "date_range = pd.date_range(start=sl.index.min(), end=sl.index.max(), freq='h')\n",
    "sl = sl[~sl.index.duplicated(keep='first')]\n",
    "sl = sl.reindex(date_range, fill_value=np.nan)\n",
    "sl.interpolate(method='time', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap = pd.read_csv('dataset/DAP (1hr).csv')\n",
    "dap['timestamp'] = pd.to_datetime(dap['timestamp'])\n",
    "dap.set_index('timestamp', inplace=True)\n",
    "date_range = pd.date_range(start=dap.index.min(), end='2024-01-01 23:55:00', freq='h')\n",
    "dap = dap[~dap.index.duplicated(keep='first')]\n",
    "dap = dap.reindex(date_range, fill_value=np.nan)\n",
    "dap.interpolate(method='time', inplace=True)\n",
    "# 24-hour average (24 hours * 1 reading per hour = 24 samples)\n",
    "dap['avg_24h'] = dap['SystemLambda'].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "# 7-day average (7 days * 24 hours = 168 samples)\n",
    "dap['avg_7d'] = dap['SystemLambda'].rolling(window=168, min_periods=1).mean()\n",
    "\n",
    "dap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_fuel = pd.read_csv('dataset/Fuel (5m).csv')\n",
    "current_fuel['interval_start_local'] = pd.to_datetime(current_fuel['interval_start_local'])\n",
    "current_fuel.set_index('interval_start_local', inplace=True)\n",
    "date_range = pd.date_range(start=current_fuel.index.min(), end='2024-01-01 23:55:00', freq='h')\n",
    "current_fuel = current_fuel[~current_fuel.index.duplicated(keep='first')]\n",
    "current_fuel = current_fuel.reindex(date_range, fill_value=np.nan)\n",
    "current_fuel.interpolate(method='time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_renewables = current_fuel.copy()\n",
    "current_renewables[\"renewable\"] = (\n",
    "    current_renewables[\"solar\"] +\n",
    "    current_renewables[\"hydro\"] +\n",
    "    current_renewables[\"wind\"]\n",
    ")\n",
    "current_renewables = current_renewables[[\"renewable\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_load = pd.read_csv('dataset/Load (5m).csv')\n",
    "current_load['interval_start_local'] = pd.to_datetime(current_load['interval_start_local'])\n",
    "current_load.set_index('interval_start_local', inplace=True)\n",
    "date_range = pd.date_range(start=current_load.index.min(), end='2024-01-01 23:55:00', freq='h')\n",
    "current_load = current_load[~current_load.index.duplicated(keep='first')]\n",
    "current_load = current_load.reindex(date_range, fill_value=np.nan)\n",
    "current_load.interpolate(method='time', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_nonrenew = pd.DataFrame()\n",
    "current_nonrenew[\"non-renew\"] = current_load[\"load\"] - current_renewables[\"renewable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted_tlf = pd.read_csv('dataset/FORECASTED_TLF (15m).csv')\n",
    "forecasted_tlf['datetime'] = pd.to_datetime(forecasted_tlf['datetime'])\n",
    "forecasted_tlf.set_index('datetime', inplace=True)\n",
    "date_range = pd.date_range(start=forecasted_tlf.index.min(), end='2024-01-01 23:55:00', freq='h')\n",
    "forecasted_tlf = forecasted_tlf[~forecasted_tlf.index.duplicated(keep='first')]\n",
    "forecasted_tlf = forecasted_tlf.reindex(date_range, fill_value=np.nan)\n",
    "forecasted_tlf.interpolate(method='time', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file for natural gas prices\n",
    "ng_price = pd.read_csv('dataset/NaturalGasPrice (1D).csv')\n",
    "\n",
    "# Convert the 'datetime' column to datetime objects\n",
    "ng_price['datetime'] = pd.to_datetime(ng_price['Date'])\n",
    "\n",
    "# Set the 'datetime' column as the index\n",
    "ng_price.set_index('datetime', inplace=True)\n",
    "\n",
    "# Create an hourly date range from the minimum datetime up to '2024-01-01 23:55:00'\n",
    "date_range = pd.date_range(start=ng_price.index.min(), end='2024-01-01 23:55:00', freq='h')\n",
    "\n",
    "# Remove duplicated datetime entries, keeping the first occurrence\n",
    "ng_price = ng_price[~ng_price.index.duplicated(keep='first')]\n",
    "\n",
    "# Reindex the DataFrame to the hourly date range, filling missing entries with NaN\n",
    "ng_price = ng_price.reindex(date_range, fill_value=np.nan)\n",
    "\n",
    "# Interpolate missing values using time-based interpolation\n",
    "ng_price.interpolate(method='time', inplace=True)\n",
    "ng_price.drop(columns=['Date'], inplace=True)\n",
    "ng_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataframe\n",
    "df = pd.concat([dap, sl, forecasted_tlf, ng_price, current_nonrenew], axis=1)\n",
    "\n",
    "# Rename columns (adding Natural Gas at the end)\n",
    "df.columns = ['DAP', 'DAP 24H', 'DAP 7D', 'SCED', 'F_TLF', 'NG Price', 'Non-Renew']\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import holidays\n",
    "\n",
    "# Get official Texas holidays\n",
    "tx_holidays = holidays.US(state='TX')  # includes state + federal holidays observed in Texas\n",
    "\n",
    "# Add holiday indicator to your DataFrame (assumes datetime index)\n",
    "df['is_holiday'] = df.index.to_series().apply(lambda x: 1 if x.date() in tx_holidays else 0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## log transform\n",
    "log_data = df.copy(deep=True)\n",
    "\n",
    "columns_to_log = ['DAP', 'DAP 24H', 'DAP 7D', 'SCED', 'F_TLF', 'NG Price', 'Non-Renew']\n",
    "\n",
    "offsets = {}           # column → min value\n",
    "for col in columns_to_log:\n",
    "    offsets[col] = df[col].min()\n",
    "    log_data[col] = np.log(df[col] - offsets[col] + 1)\n",
    "\n",
    "# 3 years training, 1 year validation, 1 year testing\n",
    "x_train_df_reg = log_data.loc[:'2021-12-31 23:55:00'].iloc[:, :]\n",
    "x_val_df_reg = log_data.loc['2022-01-01 00:00:00':'2022-12-31 23:55:00'].iloc[:, :]\n",
    "x_test_df_reg = log_data.loc['2023-01-01 00:00:00':].iloc[:, :]\n",
    "\n",
    "# Change target variable to DAP\n",
    "y_train_df_reg = log_data.loc[:'2021-12-31 23:55:00'].iloc[:, :1]\n",
    "y_val_df_reg = log_data.loc['2022-01-01 00:00:00':'2022-12-31 23:55:00'].iloc[:, :1]\n",
    "y_test_df_reg = log_data.loc['2023-01-01 00:00:00':].iloc[:, :1]\n",
    "\n",
    "x_train_df_reg.reset_index(drop=True, inplace=True)\n",
    "x_val_df_reg.reset_index(drop=True, inplace=True)\n",
    "x_test_df_reg.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Standardization\n",
    "x_mean_reg, x_std_reg = x_train_df_reg.mean(), x_train_df_reg.std()\n",
    "y_mean_reg, y_std_reg = y_train_df_reg.mean(), y_train_df_reg.std()\n",
    "\n",
    "x_std_reg = x_std_reg + 0.00001\n",
    "\n",
    "x_train_reg = (x_train_df_reg - x_mean_reg) / x_std_reg\n",
    "x_val_reg = (x_val_df_reg - x_mean_reg) / x_std_reg\n",
    "x_test_reg = (x_test_df_reg - x_mean_reg) / x_std_reg\n",
    "\n",
    "y_train_reg = (y_train_df_reg - y_mean_reg) / y_std_reg\n",
    "y_val_reg = (y_val_df_reg - y_mean_reg) / y_std_reg\n",
    "y_test_reg = (y_test_df_reg - y_mean_reg) / y_std_reg\n",
    "\n",
    "# Shift the data for the lags\n",
    "n_steps_in = 24\n",
    "n_steps_out = 24\n",
    "\n",
    "x_train_lstm = np.array([x_train_reg[i:i+n_steps_in] for i in range(0, x_train_reg.shape[0]-n_steps_in-n_steps_out+1)])\n",
    "y_train_lstm = np.array([y_train_reg[i+n_steps_in:i+n_steps_in+n_steps_out] for i in range(0, y_train_reg.shape[0]-n_steps_in-n_steps_out+1)])\n",
    "\n",
    "x_val_lstm = np.array([x_val_reg[i:i+n_steps_in] for i in range(0, x_val_reg.shape[0]-n_steps_in-n_steps_out+1)])\n",
    "y_val_lstm = np.array([y_val_reg[i+n_steps_in:i+n_steps_in+n_steps_out] for i in range(0, y_val_reg.shape[0]-n_steps_in-n_steps_out+1)])\n",
    "\n",
    "x_test_lstm = np.array([x_test_reg[i:i+n_steps_in] for i in range(0, x_test_reg.shape[0]-n_steps_in-n_steps_out+1)])\n",
    "y_test_lstm = np.array([y_test_reg[i+n_steps_in:i+n_steps_in+n_steps_out] for i in range(0, y_test_reg.shape[0]-n_steps_in-n_steps_out+1)])\n",
    "\n",
    "print(x_train_lstm.shape, y_train_lstm.shape, x_val_lstm.shape, y_val_lstm.shape, x_test_lstm.shape, y_test_lstm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lstm = y_train_lstm.squeeze(-1)\n",
    "y_val_lstm = y_val_lstm.squeeze(-1)\n",
    "y_test_lstm = y_test_lstm.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a directory for saving the model if it doesn't exist\n",
    "model_dir = os.path.join(os.getcwd(), 'saved_model')\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=dropout_rate)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = out[:, -1, :]        # take output from last time step\n",
    "        out = self.dropout(out)    # apply dropout before dense layers\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters (same as before)\n",
    "n_neurons = 64\n",
    "learning_rate = 0.0005\n",
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "patience = 50  # for early stopping\n",
    "\n",
    "input_size = x_train_lstm.shape[2]  # number of features\n",
    "output_size = y_train_lstm.shape[1]  # number of time steps out (24)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = LSTMModel(input_size, n_neurons, output_size, dropout_rate=0.3).to(device)\n",
    "criterion = nn.HuberLoss(delta=1.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_dataset = TensorDataset(torch.tensor(x_train_lstm, dtype=torch.float32),\n",
    "                              torch.tensor(y_train_lstm, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(torch.tensor(x_val_lstm, dtype=torch.float32),\n",
    "                            torch.tensor(y_val_lstm, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_val_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch_x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item() * batch_x.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Check early stopping condition\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(model_dir, 'DAP_Model_Huber.pt'))\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the best model\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, 'DAP_Model_Huber.pt')))\n",
    "\n",
    "# Make predictions on the test set\n",
    "model.eval()\n",
    "test_dataset = TensorDataset(torch.tensor(x_test_lstm, dtype=torch.float32),\n",
    "                              torch.tensor(y_test_lstm, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "y_test_pred = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, _ in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        y_test_pred.append(outputs.cpu().numpy())\n",
    "y_test_pred = np.concatenate(y_test_pred, axis=0)\n",
    "\n",
    "# Define an evaluation function\n",
    "def evaluate_prediction(predictions, actual):\n",
    "    errors = predictions - actual\n",
    "    mse = np.square(errors).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.abs(errors).mean()\n",
    "    print('MAE: {:.2f}'.format(mae))\n",
    "    print('RMSE: {:.2f}'.format(rmse))\n",
    "    return mae, rmse\n",
    "\n",
    "print(\"Evaluation on normalized scale:\")\n",
    "evaluate_prediction(y_test_pred, y_test_lstm)\n",
    "\n",
    "# Rescale predictions back to the original scale (using your stored standardization parameters)\n",
    "y_test_pred_rescale = y_test_pred * y_std_reg.values + y_mean_reg.values\n",
    "y_test_lstm_rescale = y_test_lstm * y_std_reg.values + y_mean_reg.values\n",
    "\n",
    "# Inverse log transformation to get actual values\n",
    "dap_offset = offsets[\"DAP\"]\n",
    "\n",
    "y_test_pred_invlog = np.exp(y_test_pred_rescale) - 1 + dap_offset\n",
    "y_test_lstm_invlog = np.exp(y_test_lstm_rescale) - 1 + dap_offset\n",
    "\n",
    "print(\"Evaluation after rescaling and inverse log:\")\n",
    "evaluate_prediction(y_test_pred_invlog, y_test_lstm_invlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add timestamp to the predictions\n",
    "sl_actual = pd.DataFrame(y_test_lstm_invlog[:,:], index=df.loc['2023-01-01 23:00:00':'2023-12-30 23:00:00'].index)\n",
    "sl_pred = pd.DataFrame(y_test_pred_invlog, index=df.loc['2023-01-01 23:00:00':'2023-12-30 23:00:00'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Compute baseline predictions and MSE on the test set\n",
    "with torch.no_grad():\n",
    "    x_test_tensor = torch.tensor(x_test_lstm, dtype=torch.float32, device=device)\n",
    "    baseline_pred_tensor = model(x_test_tensor)\n",
    "    baseline_pred = baseline_pred_tensor.cpu().numpy()\n",
    "\n",
    "baseline_mse = np.mean((baseline_pred - y_test_lstm)**2)\n",
    "print(\"Baseline MSE:\", baseline_mse)\n",
    "\n",
    "# Initialize a dictionary to store importance values for each feature\n",
    "feature_importance = {}\n",
    "num_features = x_test_lstm.shape[2]\n",
    "feature_names = [\n",
    "    'DAP',\n",
    "    'DAP 24H',\n",
    "    'DAP 7D' ,\n",
    "    'SCED',\n",
    "    'F_TLF',\n",
    "    'NG Price',\n",
    "    'Non-Renew',\n",
    "    'is_holiday',\n",
    "]\n",
    "\n",
    "\n",
    "# Loop over each feature and measure the impact of shuffling its values\n",
    "for feature in range(num_features):\n",
    "    # Copy the test data\n",
    "    x_test_permuted = x_test_lstm.copy()\n",
    "    # Shuffle the current feature's values for each time step across samples\n",
    "    for t in range(x_test_lstm.shape[1]):\n",
    "        x_test_permuted[:, t, feature] = np.random.permutation(x_test_permuted[:, t, feature])\n",
    "    \n",
    "    # Convert the permuted data to a torch tensor and get predictions\n",
    "    with torch.no_grad():\n",
    "        x_test_permuted_tensor = torch.tensor(x_test_permuted, dtype=torch.float32, device=device)\n",
    "        permuted_pred_tensor = model(x_test_permuted_tensor)\n",
    "        permuted_pred = permuted_pred_tensor.cpu().numpy()\n",
    "    \n",
    "    permuted_mse = np.mean((permuted_pred - y_test_lstm)**2)\n",
    "    # Importance is the increase in MSE due to permutation\n",
    "    importance = permuted_mse - baseline_mse\n",
    "    feature_importance[feature_names[feature]] = importance\n",
    "\n",
    "print(\"Feature Importance (increase in MSE):\")\n",
    "for feature, imp in feature_importance.items():\n",
    "    print(f\"{feature}: {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the first row from sl_pred and sl_actual\n",
    "predicted = sl_pred.iloc[2500].values\n",
    "actual = sl_actual.iloc[2500].values\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 25), predicted, label='Predicted', marker='o')\n",
    "plt.plot(range(1, 25), actual, label='Actual', marker='x')\n",
    "plt.title(\"24-Hour Forecast Combo Model\")\n",
    "plt.xlabel(\"Forecast Hour Ahead\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 0.  Small helpers\n",
    "# ----------------------------------------------------------------\n",
    "def to_numpy(t):\n",
    "    \"\"\"Detach, move to CPU, NumPy.\"\"\"\n",
    "    return t.detach().cpu().numpy()\n",
    "\n",
    "# --- everywhere you invert -------------------------------------\n",
    "def inverse_transform(log_scaled):\n",
    "    rescaled = log_scaled * y_std_reg.values + y_mean_reg.values\n",
    "    return np.exp(rescaled) - 1 + offsets[\"DAP\"]        # <- use the right shift\n",
    "\n",
    "def make_residuals(x_seq, y_seq):\n",
    "    \"\"\"Return actual prices, baseline preds, and residuals (all original scale).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(x_seq, dtype=torch.float32, device=device))\n",
    "    preds_orig = inverse_transform(to_numpy(preds))\n",
    "    y_orig     = inverse_transform(y_seq)\n",
    "    return y_orig, preds_orig, y_orig - preds_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tv_orig,  base_tv_pred,  res_tv  = make_residuals(\n",
    "    np.concatenate([x_train_lstm, x_val_lstm]),\n",
    "    np.concatenate([y_train_lstm, y_val_lstm])\n",
    ")\n",
    "y_test_orig, base_test_pred, res_test = make_residuals(x_test_lstm, y_test_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 2.  Label spikes\n",
    "#     Rule: |residual| > Q3 + 1.5·IQR   (tune iqr_mult if needed)\n",
    "# ----------------------------------------------------------------\n",
    "iqr_mult = 1.5\n",
    "q1, q3   = np.percentile(np.abs(res_tv), [25, 75])\n",
    "threshold = q3 + iqr_mult * (q3 - q1)\n",
    "print(f\"Spike threshold = {threshold:,.2f} $/MWh\")\n",
    "\n",
    "spike_flag_tv   = (np.abs(res_tv)   > threshold).astype(int)   # (N_seq, 24)\n",
    "spike_flag_test = (np.abs(res_test) > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 3.  Flatten each 24‑step sequence so every hour is one sample\n",
    "#     (feature vector = most‑recent hour of the window)\n",
    "# ----------------------------------------------------------------\n",
    "def flatten_features(x_seq):\n",
    "    \"\"\"Return X of shape (N_seq*24, n_features) using last hour of each window.\"\"\"\n",
    "    X_last = x_seq[:, -1, :]                # (N_seq, n_features)\n",
    "    return np.repeat(X_last, 24, axis=0)    # repeat for 24 hrs / seq\n",
    "\n",
    "def flatten_labels(label_seq):\n",
    "    \"\"\"Flatten (N_seq, 24) ➔ (N_seq*24,)\"\"\"\n",
    "    return label_seq.reshape(-1)\n",
    "\n",
    "X_tv   = flatten_features(np.concatenate([x_train_lstm, x_val_lstm]))\n",
    "X_test = flatten_features(x_test_lstm)\n",
    "y_flag_tv  = flatten_labels(spike_flag_tv)\n",
    "y_flag_te  = flatten_labels(spike_flag_test)\n",
    "y_size_tv  = flatten_labels(res_tv)\n",
    "\n",
    "X_flag_tr, X_flag_val, y_flag_tr, y_flag_val = train_test_split(\n",
    "    X_tv, y_flag_tv, test_size=0.2, stratify=y_flag_tv, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# start with 10 trees, then add 10 more each loop (warm_start=True keeps the old trees)\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=10,           # will grow to 300\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    warm_start=True            # <‑‑ key: lets us add trees incrementally\n",
    ")\n",
    "\n",
    "max_estimators   = 300         # final model size\n",
    "step             = 10          # “epoch” size\n",
    "best_val_acc     = 0.0\n",
    "\n",
    "for n in range(10, max_estimators + 1, step):\n",
    "    t0 = time()\n",
    "    clf.fit(X_flag_tr, y_flag_tr)                 # adds (n_estimators‑prev) new trees\n",
    "    val_acc = clf.score(X_flag_val, y_flag_val)\n",
    "    elapsed = time() - t0\n",
    "    \n",
    "    print(f\"Trees: {n:3d} | Val Acc: {val_acc:.3f} | Δt: {elapsed:5.1f}s\")\n",
    "    \n",
    "    # simple early‑stopping example (optional)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_n       = n\n",
    "    elif n - best_n >= 50:      # no improvement for 50 trees ≈ 5 “epochs”\n",
    "        print(\"Early‑stopping: validation accuracy plateaued.\")\n",
    "        break\n",
    "    \n",
    "    # prepare for next epoch\n",
    "    clf.n_estimators += step    # grow the ensemble by another 10 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_flag_tr, y_flag_tr)\n",
    "print(f\"Spike‑classifier validation accuracy: {clf.score(X_flag_val, y_flag_val):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 5.  Train spike‑size regressor  — incremental “epochs” of 10 trees\n",
    "# ----------------------------------------------------------------\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# --- keep only rows that are spikes -----------------------------------------\n",
    "spike_rows = y_flag_tv == 1\n",
    "X_size = X_tv[spike_rows]\n",
    "y_size = y_size_tv[spike_rows]\n",
    "\n",
    "# --- split into train / validation ------------------------------------------\n",
    "X_sz_tr, X_sz_val, y_sz_tr, y_sz_val = train_test_split(\n",
    "    X_size, y_size, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- incremental GBM ---------------------------------------------------------\n",
    "reg = GradientBoostingRegressor(\n",
    "    n_estimators=10,            # will grow to 400\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    warm_start=True             # <‑‑ allows incremental growth\n",
    ")\n",
    "\n",
    "max_estimators = 400\n",
    "step           = 10\n",
    "best_val_mae   = float(\"inf\")\n",
    "\n",
    "for n in range(10, max_estimators + 1, step):\n",
    "    t0 = time()\n",
    "    reg.fit(X_sz_tr, y_sz_tr)                     # adds new trees\n",
    "    val_mae = mean_absolute_error(y_sz_val, reg.predict(X_sz_val))\n",
    "    elapsed = time() - t0\n",
    "    \n",
    "    print(f\"Trees: {n:3d} | Val MAE: {val_mae:7.3f} | Δt: {elapsed:5.1f}s\")\n",
    "    \n",
    "    # simple early‑stopping rule (optional)\n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        best_n       = n\n",
    "    elif n - best_n >= 50:          # no improvement for 50 trees ≈ 5 “epochs”\n",
    "        print(\"Early‑stopping: validation MAE plateaued.\")\n",
    "        break\n",
    "    \n",
    "    # prepare for next epoch\n",
    "    reg.n_estimators += step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_tv[spike_rows], y_size_tv[spike_rows])\n",
    "print(f\"Spike‑size regressor trained on {spike_rows.sum()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 6.  Combine on the test set\n",
    "# ----------------------------------------------------------------\n",
    "# -- predict spike probability and hard 0/1 flag\n",
    "flag_prob = clf.predict_proba(X_test)[:, 1]\n",
    "flag_pred = (flag_prob > 0.5).astype(int)\n",
    "\n",
    "# -- predict spike size (signed); ignore where no spike predicted\n",
    "size_pred = reg.predict(X_test)\n",
    "size_pred[flag_pred == 0] = 0.0\n",
    "\n",
    "# -- reshape back to (N_seq, 24)\n",
    "flag_pred = flag_pred.reshape(base_test_pred.shape)\n",
    "size_pred = size_pred.reshape(base_test_pred.shape)\n",
    "\n",
    "final_pred = base_test_pred + size_pred  # baseline + spike correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 7.  Evaluation\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "print(\"\\nPerformance on 2023 test set (original $/MWh scale)\")\n",
    "print(\"\\nBaseline LSTM\")\n",
    "evaluate_prediction(base_test_pred,  y_test_orig)\n",
    "print(\"\\nWith Spikes\")\n",
    "evaluate_prediction(final_pred,      y_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test = df.loc['2023-01-01 23:00:00':'2023-12-30 23:00:00'].index\n",
    "\n",
    "sl_actual       = pd.DataFrame(y_test_orig,      index=index_test)\n",
    "sl_pred         = pd.DataFrame(base_test_pred,   index=index_test)\n",
    "sl_pred_spike   = pd.DataFrame(final_pred,       index=index_test)   # <‑‑ NEW\n",
    "\n",
    "print(\"Shapes  |  actual:\", sl_actual.shape,\n",
    "      \"|  baseline:\", sl_pred.shape,\n",
    "      \"|  with‑spikes:\", sl_pred_spike.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first row from sl_pred and sl_actual\n",
    "spike = sl_pred_spike.iloc[5722].values\n",
    "predicted = sl_pred.iloc[5722].values\n",
    "actual = sl_actual.iloc[5722].values\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 25), spike, label='Spike Model', marker='o')\n",
    "plt.plot(range(1, 25), predicted, label='Baseline', marker='o')\n",
    "plt.plot(range(1, 25), actual, label='Actual', marker='x')\n",
    "plt.title(\"24-Hour Forecast Combo Model\")\n",
    "plt.xlabel(\"Forecast Hour Ahead\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"spikes predicted:\", np.count_nonzero(flag_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(size_pred).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  TOP‑20 LARGEST SPIKE CORRECTIONS  (no extra helpers required)\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- make sure the three DataFrames exist -----------------------\n",
    "for name in (\"sl_actual\", \"sl_pred\", \"sl_pred_spike\"):\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"Missing DataFrame: {name}. \"\n",
    "                           \"Run the baseline and spike‑pipeline cells first.\")\n",
    "\n",
    "# --- 1. absolute correction matrix ------------------------------\n",
    "abs_corr = (sl_pred_spike - sl_pred).abs()\n",
    "\n",
    "# --- 2. flatten and take the 20 largest -------------------------\n",
    "flat_corr = abs_corr.stack()                      # MultiIndex (timestamp, lead_hour)\n",
    "top20 = flat_corr.nlargest(20)\n",
    "\n",
    "# --- 3. build a tidy table --------------------------------------\n",
    "records = []\n",
    "for rank, ((ts_label, lead_hour), delta) in enumerate(top20.items(), start=1):\n",
    "    row_pos      = sl_actual.index.get_loc(ts_label)         # integer position\n",
    "    start_time   = ts_label                                  # forecast issued\n",
    "    target_time  = start_time + pd.Timedelta(hours=lead_hour)\n",
    "\n",
    "    baseline_val = sl_pred.iat[row_pos,   lead_hour]\n",
    "    spike_val    = sl_pred_spike.iat[row_pos, lead_hour]\n",
    "    actual_val   = sl_actual.iat[row_pos, lead_hour]\n",
    "\n",
    "    records.append({\n",
    "        \"Rank\":                rank,\n",
    "        \"Issued\":              start_time,\n",
    "        \"Lead‑Hour\":           lead_hour + 1,                # 1‑based\n",
    "        \"Target Time\":         target_time,\n",
    "        \"Baseline ($/MWh)\":    baseline_val,\n",
    "        \"Spike Model ($/MWh)\": spike_val,\n",
    "        \"Actual ($/MWh)\":      actual_val,\n",
    "        \"Abs Correction\":      delta\n",
    "    })\n",
    "\n",
    "df_top20 = pd.DataFrame(records)\n",
    "\n",
    "# --- 4. pretty‑print --------------------------------------------\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.float_format\", \"{:,.2f}\".format)\n",
    "print(df_top20.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of spikes in test set:\", spike_flag_test.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
